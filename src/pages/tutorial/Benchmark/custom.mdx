---
title: 'Custom Benchmark'
sidebar_position: 2
---

<h2 style="font-size: 2rem; font-weight: bold; margin-bottom: 1rem;">Evaluator</h2>

<p style="font-size: 1.125rem; margin-bottom: 1rem;">
In ChEF, all evaluation pipelines are managed by the <code>Evaluator</code> (<code>src/ChEF/evaluator.py</code>) class. This class serves as the control center for evaluation tasks and incorporates various components, including a scenario, an instruction, an inferencer, and a metric. These components are defined through recipe configurations.
</p>

<h3 style="font-size: 1.5rem; font-weight: bold; margin-bottom: 1rem; margin-top: 1rem;">Key Components:</h3>
<ul style="font-size: 1.125rem; margin-bottom: 1rem;">
  <li><b>Scenario:</b> The scenario represents the evaluation dataset and task-specific details.</li>
  <li><b>Instruction:</b> Responsible for processing samples and generating queries.</li>
  <li><b>Inferencer:</b> Performs model inference on the dataset.</li>
  <li><b>Metric:</b> Evaluates model performance using defined metrics.</li>
</ul>

<h2 style="font-size: 2rem; font-weight: bold; margin-bottom: 1rem; margin-top: 2rem;">Evaluation Workflow</h2>

<p style="font-size: 1.125rem; margin-bottom: 1rem;">
The evaluation process in ChEF follows a structured workflow:
</p>

<ol style="font-size: 1.125rem; margin-bottom: 1rem;">
  <li><b>Model and Data Loading:</b> First, the model and evaluation dataset (scenario) are loaded.</li>
  <li><b>Evaluator.evaluate Method:</b> The evaluation is initiated by calling the <code>evaluate</code> method of the <code>Evaluator</code> class.</li>
  <li><b>Inference with inferencer.inference:</b> The <code>inferencer</code> is used to perform model inference. During dataset traversal, the <code>InstructionHandler</code> processes each sample, generating queries that serve as inputs to the model.</li>
  <li><b>Results Saving:</b> The output of the inference is saved in the specified <code>results_path</code>.</li>
  <li><b>Metric Evaluation:</b> Finally, the <code>metric</code> evaluates the results file, calculating various performance metrics specific to the evaluation task.</li>
  <li><b>Output Evaluation Results:</b> The final evaluation results are provided as output, allowing you to assess the model's performance.</li>
</ol>

<h2 style="font-size: 2rem; font-weight: bold; margin-bottom: 1rem; margin-top: 2rem;">Employ Your Model</h2>

<p style="font-size: 1.125rem; margin-bottom: 1rem;">
In ChEF, you can employ your own custom models by following these steps:
</p>

<h3 style="font-size: 1.5rem; font-weight: bold; margin-bottom: 1rem;">Step 1: Prepare Your Model Files</h3>
<ol style="font-size: 1.125rem; margin-bottom: 1rem;">
<li>Navigate to the <code>src/ChEF/models/</code> folder in ChEF.</li>
<li>Paste all the necessary files for your custom model into this folder.</li>
</ol>

<h3 style="font-size: 1.5rem; font-weight: bold; margin-bottom: 1rem;">Step 2: Write the Test Model</h3>
<ol style="font-size: 1.125rem; margin-bottom: 1rem;">
<li>Create a new Python file in the models folder and name it something like <code>test_your_model.py</code>.</li>
<li>In this file, you will need to inherit from the <code>TestBase</code> class defined in <code>src/ChEF/models/test_base.py</code>. The <code>TestBase</code> class provides a set of interfaces that you should implement for testing your model.</li>
</ol>

<h3 style="font-size: 1.5rem; font-weight: bold; margin-bottom: 1rem;">Step 3: Test Your Model</h3>
<ol style="font-size: 1.125rem; margin-bottom: 1rem;">
<li>Add your model in <code>src/ChEF/models/__init__.py</code>.</li>
<li>Prepare your model configuration in <code>src/config/ChEF/models/</code>. For example, the config for <code>KOSMOS-2</code> (<code>src/config/ChEF/models/kosmos2.yaml</code>):</li>
</ol>

```yaml
model_name: Kosmos2
model_path: ../model_zoo/kosmos/kosmos-2.pt
if_grounding: False # set True for detection and grounding evaluation
```

<p style="font-size: 1.125rem; margin-bottom: 1rem;">
The config for KOSMOS-2 on detection tasks evaluation:
</p>

```yaml
model_name: Kosmos2
model_path: ../model_zoo/kosmos/kosmos-2.pt
if_grounding: True
```

<p style="font-size: 1.125rem; margin-bottom: 1rem;">
Use the provided recipes for evaluation:
</p>

```bash
python tools/eval.py --model_cfg configs/ChEF/models/your_model.yaml --recipe_cfg recipe_cfg
```

<h2 style="font-size: 2rem; font-weight: bold; margin-bottom: 1rem; margin-top: 2rem;">Instruction</h2>

<p style="font-size: 1.125rem; margin-bottom: 1rem;">
In ChEF, the <code>InstructionHandler</code> (<code>src/ChEF/instruction/__init__.py</code>) class plays a central role in managing instructions for generating queries when iterating through the dataset in the <code>inferencer</code>. These queries are then used as inputs to the model for various tasks.
</p>

<p style="font-size: 1.125rem; margin-bottom: 1rem;">
ChEF supports three main query types: <code>standard query</code>, <code>query pool</code>, and <code>multiturn query</code>. For each query type, various query statements are defined based on the dataset's task type.
</p>

<ol style="font-size: 1.125rem; margin-bottom: 1rem;">
<li><b>Standard Query:</b> Uses the first query defined in the query pool.</li>
<li><b>Query Pool:</b> Specifies queries in the pool by assigned ids defined in the configuration.</li>
<li><b>Multiturn Query:</b> Can get different queries depending on the turn id, which are also defined in the query pool.</li>
</ol>

<p style="font-size: 1.125rem; margin-bottom: 1rem;">
For more details, refer to the <code>src/ChEF/instruction/query.py</code>.
</p>

<p style="font-size: 1.125rem; margin-bottom: 1rem;">
<code>InstructionHandler</code> also supports generating in-context examples for queries using <code>ice_retriever</code> (<code>src/ChEF/instruction/ice_retriever/</code>). ChEF supports four types of <code>ice_retrievers</code>: <code>random</code>, <code>fixed</code>, <code>topk_text</code>, and <code>topk_img</code>. The <code>generate_ices</code> function in the <code>InstructionHandler</code> class outputs several in-context examples for the input query.
</p>

<p style="font-size: 1.125rem; margin-bottom: 1rem;">
<b>Employ Your Instruction:</b> You can add special queries in the <code>Query Pool</code>, and define the assigned ids in the recipe configuration to use the new queries. You can also define a new type of query by defining the query in <code>src/ChEF/instruction/query.py</code> and adding a new function in <code>InstructionHandler</code>.
</p>

<h2 style="font-size: 2rem; font-weight: bold; margin-bottom: 1rem; margin-top: 2rem;">Inferencer</h2>

<p style="font-size: 1.125rem; margin-bottom: 1rem;">
In ChEF, the <code>Inferencer</code> component is a crucial part of the system, responsible for model inference. ChEF offers a variety of pre-defined inferencers to cater to different needs. You can easily choose the appropriate inferencer by specifying the inferencer category and necessary settings in the recipe configuration. Additionally, users have the flexibility to define their custom inferencers.
</p>

<h3 style="font-size: 1.5rem; font-weight: bold; margin-bottom: 1rem;">Pre-Defined Inferencers:</h3>

<p style="font-size: 1.

125rem; margin-bottom: 1rem;">
ChEF provides eight different inferencers that cover a range of use cases. You can effortlessly use the desired inferencer by specifying its category and required settings in the recipe configuration.
</p>

<h3 style="font-size: 1.5rem; font-weight: bold; margin-bottom: 1rem;">Custom Inferencers:</h3>

<p style="font-size: 1.125rem; margin-bottom: 1rem;">
For advanced users and specific requirements, ChEF offers the option to create custom inferencers. The basic structure of an inferencer is defined in the <code>src/ChEF/inferencer/Direct.py</code> file (<code>Direct_inferencer</code>). You can extend this structure to implement your custom inferencer logic.
</p>

```python
class Your_inferencer(Direct_inferencer):
    def __init__(self, **kwargs) -> None:
        super().__init__(**kwargs)
        
    def inference(self, model, dataset):
        predictions = []
        # Step 1: build dataloader
        dataloader = DataLoader(dataset, batch_size=self.batch_size, collate_fn=lambda batch: {key: [dict[key] for dict in batch] for key in batch[0]})
        for batch in tqdm(dataloader, desc="Running inference"):
            # Step 2: get input query
            prompts = self.instruction_handler.generate(batch)
            # Step 3: model outputs
            outputs = model.generate(prompts)
            # Step 4: save results
            predictions = predictions + outputs
        # Step 5: output file
        self._after_inference_step(predictions)
```

<h2 style="font-size: 2rem; font-weight: bold; margin-bottom: 1rem; margin-top: 2rem;">Metric</h2>

<p style="font-size: 1.125rem; margin-bottom: 1rem;">
In ChEF, the <code>Metric</code> component plays a crucial role in evaluating and measuring the performance of models across various scenarios and protocols. ChEF offers a wide range of pre-defined metrics, each tailored to different evaluation needs. Detailed information about these metrics can be found in the <code>src/ChEF/metric/__init__.py</code> file.
</p>

<h3 style="font-size: 1.5rem; font-weight: bold; margin-bottom: 1rem;">Custom Metrics:</h3>

<p style="font-size: 1.125rem; margin-bottom: 1rem;">
ChEF also allows users to define their custom metrics. The basic structure of a metric is defined in the <code>src/ChEF/metric/utils.py</code> file (<code>Base_Metric</code>). You can extend this structure to implement your custom metric logic.
</p>

```python
class Your_metric(Base_metric):
    def __init__(self, **kwargs) -> None:
        super().__init__(**kwargs)

    def metric_func(self, answers):
        '''
            answers: List[sample], each sample is a dict
            sample: {
                'answer' : str,
                'gt_answers' : str, 
            }
        '''
        # Evaluation